{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "underlord_bot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frpB3hO-1So9",
        "colab_type": "text"
      },
      "source": [
        "Only the first 3 cells are working so far (assuming the client has the required file and path, which will be uploaded to the github asap). \n",
        "\n",
        "The other cells are just for testing algorithms but they have not been completed yet and are in no working order. \n",
        "\n",
        "The first 3 cells scrape the data on underlords characters and their synergies / health / damage etc. It contains whatever info needed (and probably some excess info based on how we use the data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZnkRtffHXAU",
        "colab_type": "code",
        "outputId": "70516b93-1f49-44ab-f73f-840e40696e9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ngf1fjxcH3me",
        "colab_type": "code",
        "outputId": "39cf4c15-c25f-4bcd-b661-71a1505a5055",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict\n",
        "from operator import itemgetter\n",
        "import numpy as np\n",
        "import tensorflow\n",
        "import itertools\n",
        "from copy import deepcopy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlqhpwuGH1qG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "fr = open('gdrive/My Drive/ML/Projects/Underlords Bot/under.html', 'r', encoding='utf8').read()\n",
        "soup = BeautifulSoup(fr, \"html.parser\")\n",
        "\n",
        "\n",
        "tags = soup('div class')\n",
        "tiers = soup('u')\n",
        "tags = soup('li')\n",
        "container = []\n",
        "for tag in tags:\n",
        "    if not tag:\n",
        "        continue\n",
        "    info = list(filter(lambda st: st != '' and '<' not in st, str(tag).split()))\n",
        "    if len(info) <= 5:\n",
        "        extract = str(tag.contents[0]).strip()\n",
        "        container.append(extract)\n",
        "        if str(tag.contents[0]).strip() == 'Lich':\n",
        "            break\n",
        "\n",
        "\n",
        "synergies = dict()\n",
        "levels = dict()\n",
        "\n",
        "synergies['S Tier'] = container[:3]\n",
        "synergies['A Tier'] = container[3:10]\n",
        "synergies['B Tier'] = container[10:15]\n",
        "synergies['C Tier'] = container[15:23]\n",
        "levels['1'] = container[23:37]\n",
        "levels['2'] = container[37:51]\n",
        "levels['3'] = container[51:66]\n",
        "levels['4'] = container[66:78]\n",
        "levels['5'] = container[78:83]\n",
        "\n",
        "fr = open('gdrive/My Drive/ML/Projects/Underlords Bot/stats.html', 'r', encoding='utf8').read()\n",
        "soup = BeautifulSoup(fr, \"html.parser\")\n",
        "\n",
        "'''\n",
        "tags = soup('u')\n",
        "for tag in tags:\n",
        "    print(tag.contents)'''\n",
        "\n",
        "\n",
        "soup = BeautifulSoup(fr, \"html.parser\")\n",
        "tags = soup('li') \n",
        "st = \"\"\n",
        "\n",
        "collection = []\n",
        "for tag in tags:\n",
        "    if tag != None and str(tag.contents[0]).startswith('<strong>'):\n",
        "        extract = [\"Alliance 1\", \"Alliance 2\", 'Alliance 3', 'Alliance 4', 'Health', 'DPS', 'Attack Range', 'Armor']\n",
        "        \n",
        "        for e in extract:\n",
        "            \n",
        "            if str(tag.contents[0]).find(e) != -1:\n",
        "                collection.append(str(tag.contents[1]).strip())\n",
        "                \n",
        "            \n",
        "            \n",
        "\n",
        "info = {}\n",
        "tags = soup('u')\n",
        "i = 0\n",
        "for tag in tags:\n",
        "    if str(tag)[3:-4] == 'Puck' or str(tag)[3:-4] == 'Dragon Knight' or str(tag)[3:-4] == 'Lycan':\n",
        "        info[str(tag)[3:-4]] = collection[i:i+8]\n",
        "        i += 8\n",
        "    else: \n",
        "        info[str(tag)[3:-4]] = collection[i:i+7]\n",
        "        i += 7\n",
        "\n",
        "\n",
        "\n",
        "l1 = []\n",
        "for nm in levels:\n",
        "    for si in levels[nm]:\n",
        "        l1 += [si]\n",
        "\n",
        "l2 = []\n",
        "for key in l1:\n",
        "    if 'Wind' in key:\n",
        "        l2.append(key)\n",
        "    if 'Nature' in key:\n",
        "        l2.append(key)\n",
        "\n",
        "st = ''\n",
        "for ao in info:\n",
        "    if 'Nature' in ao:\n",
        "         st = ao\n",
        "        \n",
        "for m in l2:\n",
        "    if 'Wind' in m:\n",
        "        info[m] = info['Wind Ranger']\n",
        "    else:\n",
        "        info[m] = info[ao]\n",
        "\n",
        "# give each hero a number for q-table access later on \n",
        "heroes_to_num = {}\n",
        "for index, hero in enumerate(info.keys()): \n",
        "    heroes_to_num[hero.strip()] = index\n",
        "\n",
        "num_to_heroes = {}\n",
        "for index, hero in enumerate(info.keys()): \n",
        "    num_to_heroes[index] = hero\n",
        "\n",
        "i = 0\n",
        "synergies_to_num = {}\n",
        "all_synergies = []\n",
        "\n",
        "for tier in synergies:\n",
        "    for synergy in synergies[tier]:\n",
        "        synergies_to_num[synergy] = i\n",
        "        all_synergies.append(synergies)\n",
        "        i += 1\n",
        "\n",
        "'''\n",
        "add_syns = []\n",
        "for key in synergies_to_num:\n",
        "    add_syns.append(key[:-1])\n",
        "for syn in add_syns:\n",
        "    synergies_to_num[syn] = synergies_to_num[key]'''\n",
        "\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# given a synergy which may not be in the dictionary, this tries to find the closest matching one\n",
        "def similarity_function(access):\n",
        "    # return the synergy string of which access most closely correlates to\n",
        "    max_sim_synergy = ''\n",
        "    max_ratio = 0\n",
        "\n",
        "    for key in synergies_to_num:\n",
        "        if SequenceMatcher(None, access, key).ratio() > max_ratio:\n",
        "            max_ratio = SequenceMatcher(None, access, key).ratio()\n",
        "            max_sim_synergy = key\n",
        "        return max_sim_synergy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb2QDKyIn7Zd",
        "colab_type": "code",
        "outputId": "058a1170-a83c-4515-9cba-82b5c85cee28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        }
      },
      "source": [
        "levels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'1': ['Drow Ranger',\n",
              "  'Axe',\n",
              "  'Tinker',\n",
              "  'Batrider',\n",
              "  'Tusk',\n",
              "  'Enchantress',\n",
              "  'Tiny',\n",
              "  'Clockwerk',\n",
              "  'Warlock',\n",
              "  'Bloodseeker',\n",
              "  'Bounty Hunter',\n",
              "  'Shadow Shaman',\n",
              "  'Anti-Mage',\n",
              "  'Ogre Magi'],\n",
              " '2': ['Witch Doctor',\n",
              "  'Chaos Knight',\n",
              "  'Queen of Pain',\n",
              "  'Pudge',\n",
              "  'Crystal Maiden',\n",
              "  'Treant Protector',\n",
              "  'Beastmaster',\n",
              "  'Timbersaw',\n",
              "  'Juggernaut',\n",
              "  'Slardar',\n",
              "  'Natures Prophet',\n",
              "  'Luna',\n",
              "  'Puck',\n",
              "  'Morphling'],\n",
              " '3': ['Shadow Fiend',\n",
              "  'Slark',\n",
              "  'Viper',\n",
              "  'Lycan',\n",
              "  'Abaddon',\n",
              "  'Razor',\n",
              "  'Sand King',\n",
              "  'Sniper',\n",
              "  'Windranger',\n",
              "  'Arc Warden',\n",
              "  'Venomancer',\n",
              "  'Lina',\n",
              "  'Phantom Assassin',\n",
              "  'Terrorblade',\n",
              "  'Omniknight'],\n",
              " '4': ['Kunkka',\n",
              "  'Doom',\n",
              "  'Medusa',\n",
              "  'Lone Druid',\n",
              "  'Keeper of the Light',\n",
              "  'Troll Warlord',\n",
              "  'Mirana',\n",
              "  'Necrophos',\n",
              "  'Dragon Knight',\n",
              "  'Disruptor',\n",
              "  'Alchemist',\n",
              "  'Templar Assassin'],\n",
              " '5': ['Tidehunter', 'Enigma', 'Techies', 'Gyrocopter', 'Lich']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMfYd6OGLeSv",
        "colab_type": "text"
      },
      "source": [
        "In our game environment, we create an observation space with factors like gold, level, # of synergies, # of heroes, etc. Each synergy and hero will have its own q-value. \n",
        "\n",
        "The q-value for a particular synergy will take into account how good the synergy is, how many heroes needed per level, etc. when backpropagated. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA6fWrqB5DJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.1\n",
        "discount = 0.95\n",
        "episodes = 5000\n",
        "\n",
        "# (level 1, level 2, level 3), synergies, level\n",
        "q_table = np.random.uniform(low=-2,high=0, size=(3, 9, 10)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsRE46clwfNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "synergies_to_hero = defaultdict(list)\n",
        "for hero in info:\n",
        "  synergy1 = similarity_function(info[hero][0])\n",
        "  synergy2 = similarity_function(info[hero][1])\n",
        "  synergies_to_hero[synergies_to_num[synergy1]] += [hero]\n",
        "  synergies_to_hero[synergies_to_num[synergy2]] += [hero]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3d4pkZVoF3_",
        "colab_type": "text"
      },
      "source": [
        "Variables:\n",
        "\n",
        "synergies_to_num - encodes synergy \n",
        "\n",
        "info - maps hero names to stats such as synergies (some are inaccurate, so use similarity function)\n",
        "\n",
        "levels - maps level / tier to a list of characters\n",
        "\n",
        "hero_probs - maps hero encoding to probability of getting hero\n",
        "\n",
        "synergies_to_hero - key will be synergy (encoded?), and will map to hero encodings \n",
        "\n",
        "syns - encoded keys to count\n",
        "\n",
        "Each hero has some probability based on the tier, and they have synergies \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc0plhDHpiiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hero_probs = defaultdict(list) # probability of getting a hero \n",
        "\n",
        "\n",
        "# action will be a list of moves (strings), inventory is a dictionary of hero encodings to count\n",
        "def expected_value(actions, level, inventory, syns={}):\n",
        "  expected = 0\n",
        "\n",
        "  # if not in inventory (inventory[piece] = 0), then count the synergy as new\n",
        "  for action in actions:\n",
        "    if action != 'sell':\n",
        "      if inventory[pieces_to_num[action]] == 0:\n",
        "        synergy1 = similarity_function(info[action][0])\n",
        "        synergy2 = similarity_function(info[action][1])\n",
        "        syns[synergies_to_num[synergy1]] += 1 \n",
        "        syns[synergies_to_num[synergy2]] += 1 \n",
        "      inventory[pieces_to_num[action]] += 1\n",
        "\n",
        "  \n",
        "  for hero in info:\n",
        "    hero_probs[pieces_to_num[hero]] = prob(hero) # returns probability of getting that particular hero, based on lvl\n",
        "\n",
        "  # expected value of getting sets of pieces which gets this synergy boost\n",
        "  for syn in syns:\n",
        "    # find next closest # of synergies needed for boost for each of the synergies \n",
        "    next = next_closest(syn)\n",
        "    # find all combination of heroes which make it possible to get synergy boost\n",
        "    if next != 3:\n",
        "      for comb in itertools.combinations(synergies_to_hero[syn], next):\n",
        "        f = True\n",
        "        for item in comb:\n",
        "          if inventory[item] != 0: # probability of affecting synergy = 0\n",
        "            f = False\n",
        "        if not f: \n",
        "          continue\n",
        "        expected += 50 * (hero_probs[comb[0]] * hero_probs[comb[1]])\n",
        "  # probability of getting sets of pieces (or) which will get this synergy boost \n",
        "  # probability of leveling up the pieces already in inventory if level up\n",
        "  # probability of leveling up pieces in shop if deciding to get them\n",
        "\n",
        "  # recursive up\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMobrDVjm8RM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dictionary of piece # to synergy\n",
        "\n",
        "for round in range(5):\n",
        "  # generate troops\n",
        "  generate_troops()\n",
        "  \n",
        "  # expected value in 5 rounds using a recursive function and"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSEV4DZ0jyrw",
        "colab_type": "text"
      },
      "source": [
        "Perhaps we can choose randomly from the action space size instead of "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os95yWTCmfte",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "synergies_to_num"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfVgaj4LJ-8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tweak these\n",
        "SECOND_LVL_REWARD  = 60\n",
        "THIRD_LVL_REWARD = 180\n",
        "FIRST_SYN_REWARD = 50\n",
        "SECOND_SYN_REWARD = 130\n",
        "THIRD_SYN_REWARD = 200\n",
        "\n",
        "OBSERVATION_SPACE_VALUES = [80, 10]\n",
        "# each of these cells will contain count of a particular hero\n",
        "for p in range(60):\n",
        "  OBSERVATION_SPACE_VALUES.append(1)\n",
        "# will contain the number of each synergy\n",
        "for p in range(23):\n",
        "  OBSERVATION_SPACE_VALUES.append(1)\n",
        "OBSERVATION_SPACE_VALUES = tuple(OBSERVATION_SPACE_VALUES)\n",
        "\n",
        "# OBSERVATION_SPACE_VALUES = (80, 10 ()) # 23, 60) # gold, level, # of synergies, # of heroes, \n",
        "\n",
        "ACTION_SPACE_SIZE = 60 # each of the heroes or saving or ...\n",
        "class GameEnv:\n",
        "\n",
        "  \n",
        "  def reset(self):\n",
        "    self.pieces_to_num = heroes_to_num\n",
        "    self.num_to_pieces = num_to_heroes\n",
        "    self.synergies_to_num = synergies_to_num\n",
        "    self.piece_stats = info\n",
        "    self.gold = 1\n",
        "    self.lev = 1\n",
        "    self.xp = 1\n",
        "    self.inventory = dict()\n",
        "    self.iter = 0\n",
        "    for value in heroes_to_num.values(): \n",
        "        self.inventory[value] = [0, 0, 0] # number of level 1, 2, and 3 troops\n",
        "    self.exp_needed = [2, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40] # experience needed to level up\n",
        "    self.synergy_inventory = dict()\n",
        "    for n in range(i):\n",
        "        self.synergy_inventory[n] = 0\n",
        "    self.existing = {}\n",
        "    self.rnd = 1\n",
        "    self.episode_step = 0\n",
        "  \n",
        "  \n",
        "  def __init__(self):\n",
        "    self.pieces_to_num = heroes_to_num\n",
        "    self.num_to_pieces = num_to_heroes\n",
        "    self.synergies_to_num = synergies_to_num\n",
        "    self.piece_stats = info\n",
        "    self.gold = 1\n",
        "    self.lev = 1\n",
        "    self.xp = 1\n",
        "    self.inventory = dict()\n",
        "    self.iter = 0\n",
        "    for value in heroes_to_num.values(): \n",
        "        self.inventory[value] = [0, 0, 0] # number of level 1, 2, and 3 troops\n",
        "    self.exp_needed = [2, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40] # experience needed to level up\n",
        "    self.synergy_inventory = dict()\n",
        "    for n in range(i):\n",
        "        self.synergy_inventory[n] = 0\n",
        "    self.existing = {}\n",
        "    self.rnd = 1\n",
        "    self.episode_step = 0\n",
        "  \n",
        "  # updates level of pieces on board if there are enough\n",
        "  def update_level(piece, index):\n",
        "    if self.inventory[self.pieces_to_num[piece]][index] == 3:\n",
        "      self.inventory[self.pieces_to_num[piece]][index] = 0\n",
        "      self.inventory[self.pieces_to_num[piece]][index + 1] += 1\n",
        "  \n",
        "  # choice is the action number\n",
        "  def action(choice):\n",
        "    # if a hero is bought\n",
        "    if 0 <= choice <= 59: \n",
        "      self.inventory[self.choice][0] += 1\n",
        "      \n",
        "      # take care of merging / levelling\n",
        "      update_level(self.num_to_pieces[choice], 0)\n",
        "      update_level(self.num_to_pieces[choice], 1)\n",
        "      \n",
        "      # count them as synergy\n",
        "      if self.inventory[self.choice] == [1, 0, 0]:\n",
        "        first_syn = similarity_function[info[choice][0]]\n",
        "        second_syn = similarity_function[info[choice][1]]\n",
        "        self.synergy_inventory[first_syn] += 1\n",
        "        self.synergy_inventory[second_syn] += 1\n",
        "        \n",
        "      \n",
        "      # take away gold if they bought a piece\n",
        "      for key in levels:\n",
        "        if self.num_to_pieces[choice] in levels[key]:\n",
        "          self.gold -= int(key)\n",
        "      \n",
        "      \n",
        "      \n",
        "  def step(self, action):\n",
        "    self.episode_step += 1\n",
        "    reward = 0 \n",
        "    \n",
        "    for piece in self.inventory:\n",
        "      if self.inventory[piece][1] % 1 == 0: # level 2  piece\n",
        "        reward += (SECOND_LVL_REWARD * self.inventory[piece][1])\n",
        "      elif self.inventory[piece][2] == 1:\n",
        "        reward += THIRD_LVL_REWARD\n",
        "    \n",
        "    for synergy in self.synergy_inventory:\n",
        "      if self.synergy_inventory[synergy] == 3:\n",
        "        reward += FIRST_SYN_REWARD\n",
        "        \n",
        "        \n",
        "    reward += self.gold * 2\n",
        "    \n",
        "    obs = [self.gold, self.lev]\n",
        "    \n",
        "    for m in range(60):\n",
        "      obs.append(0)\n",
        "    for m in range(23):\n",
        "      obs.append(0)\n",
        "    \n",
        "    for piece in self.inventory:\n",
        "      obs[2 + self.piece_to_num[piece]] = sum(self.inventory[piece])\n",
        "      \n",
        "    for synergy in self.synergy_inventory:\n",
        "      obs[62 + self.synergies_to_num[synergy]] = self.synergy_inventory[synergy]\n",
        "      \n",
        "    \n",
        "    if self.episode_step >= 35:\n",
        "      done = True\n",
        "      \n",
        "    return obs, reward, done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58HTrAcqdT9I",
        "colab_type": "text"
      },
      "source": [
        "We're making our own custom reinforcement learning algorithm. Basically, using a replay memory, we will average the sum of rewards over the next few rounds which is a result of purchasing a particular piece. We will then create a model which attempts to predict the sum of rewards given the state and available pieces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5F8c4JnkUMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent_model = Sequential()\n",
        "agent_model.add(Dense(100, activation='relu'))\n",
        "agent_model.add(Dense(60, activation='linear'))\n",
        "\n",
        "target_model = Sequential()\n",
        "target_model.add(Dense(100, activation='relu'))\n",
        "target_model.add(Dense(60, activation='linear'))\n",
        "\n",
        "env = GameEnv()\n",
        "\n",
        "for episode in range(35000):\n",
        "  generate_troops() # list of troops\n",
        "  agent_model.predict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1P3oIwOpnEK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = GameEnv()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbRRasYjmpuP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "class DQNAgent:\n",
        "    def create_model(self):\n",
        "        model = Sequential()\n",
        "\n",
        "        #model.add(Conv2D(256,  (3, 3), input_shape=OBSERVATION_SPACE_VALUES)) # observation space values = .x.x... \n",
        "        #model.add(Activation('relu'))\n",
        "        #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Dense(1024)) # fully connected layer\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "        # model.add(Conv2D(256, (3, 3)))\n",
        "        model.add(Activation('relu'))\n",
        "        #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "        model.add(Flatten()) # 1D feature vectors\n",
        "        model.add(Dense(64)) # fully connected layer\n",
        "\n",
        "        model.add(Dense(ACTION_SPACE_SIZE, activation='linear')) # action_space_Size = # of choices\n",
        "        model.compile(loss='mse', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def __init__(self):\n",
        "        # main network\n",
        "        self.model = self.create_model()\n",
        "\n",
        "        # the model that we query for future q-models will be more stable than the one being fitted each step\n",
        "        # this means that there is more consistnecy while training (if the weights fluctuate, q-values won't be on the same scale)\n",
        "        # target network - initially same as main network\n",
        "        self.target_model = self.create_model()\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "        # array w/ last n steps for training - also prevents fluctuations\n",
        "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
        "\n",
        "\n",
        "        self.target_update_counter = 0 # when to update target network w/ main network's weights\n",
        "    \n",
        "    # Adds step's data to a memory replay array / update replay memory\n",
        "    # (observation space, action, reward, new observation space, done)\n",
        "    def update_replay_memory(self, transition):\n",
        "        self.replay_memory.append(transition)\n",
        "\n",
        "    # queries the main neural network for q-values given the observation space\n",
        "    def get_qs(self, state):\n",
        "        return self.model.predict(np.array(state).reshape(-1, *state.shape) / 255)\n",
        "\n",
        "    \n",
        "    # trains network every step during episode\n",
        "    def train(self, terminal_state, step):\n",
        "        # only start training if we have at least a specified # of samples\n",
        "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
        "            return\n",
        "\n",
        "        # separate states and their q-values from that minibatch of memory, including the new state and the future qs of future states\n",
        "        \n",
        "        # get a minibatch of random samples from memory replay table\n",
        "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
        "\n",
        "        # get current states and query model for q-values\n",
        "        current_states = np.array(transition for transition in minibatch) / 255\n",
        "        current_qs_list = self.model.predict(current_states)\n",
        "\n",
        "        # get future states from minibatch, then query NN for q-values\n",
        "        # when using target network, query it - else use main network\n",
        "        # transition[3] is the new observation space\n",
        "        new_current_states = np.array([transition[3] for transition in minibatch]) / 255\n",
        "        future_qs_list = self.target_model.predict(new_current_states)\n",
        "    \n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        # for each observation in the minibatch\n",
        "        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
        "            # if not a terminal state (where the reward is then backpropagated), get new q from future states\n",
        "            # almost like q-learning except use only part of the equation\n",
        "            if not done:\n",
        "                max_future_q = np.max(future_qs_list[index])\n",
        "                new_q = reward + DISCOUNT * max_future_q\n",
        "            else:\n",
        "                new_q = reward\n",
        "\n",
        "            # update q vaues for given state\n",
        "            current_qs = current_qs_list[index]\n",
        "            current_qs[action] = new_q\n",
        "\n",
        "            # add current state, and current q \n",
        "            X.append(current_state)\n",
        "            y.append(current_qs)\n",
        "        # fit on all samples as one batch - log only on terminal state\n",
        "        self.model.fit(np.array(X) / 255, np.array(y), batch_size=MINIBATCH_SIZE, verbose=0\n",
        "                        , shuffle=False) \n",
        "\n",
        "        # Update target network counter every end of episode\n",
        "        '''\n",
        "        Note that the terminal state is not related to the replay memory - the model is trained every single step\n",
        "        and if the step passed in to this method is the final step / end of episode, then the target model weights are updated\n",
        "        '''\n",
        "        if terminal_state:\n",
        "            self.target_update_counter += 1\n",
        "        \n",
        "        # if counter reaches et value, update target network with weights\n",
        "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
        "            self.target_model.set_weights(self.model.get_weights)\n",
        "            self.target_update_counter = 0  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aez-sGTmdVmS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "timesteps = 35\n",
        "input_features = 85\n",
        "output_features = 60\n",
        "\n",
        "model = Sequential()\n",
        "# Input()\n",
        "# SimpleRNN input: (batch_size, timesteps, input_features)\n",
        "model.add(SimpleRNN(output_features, return_sequences=True)) \n",
        "model.add(SimpleRNN(output_features, return_sequences=True))\n",
        "model.add(SimpleRNN(output_features, return_sequences=True))\n",
        "model.add(SimpleRNN(output_features))\n",
        "model.add(Dense(1, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "# input train contains the state / features\n",
        "history = model.fit(input_train, y_train, epochs=1)\n",
        "\n",
        "inputs = ... # shape of (timesteps, input_features)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfo3x0CkpaHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Activation, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from collections import deque\n",
        "\n",
        "REPLAY_MEMORY_SIZE = 50_000\n",
        "\n",
        "epsilon = 0.8\n",
        "\n",
        "agent = DQNAgent()\n",
        "\n",
        "for episode in range(30000):\n",
        "\n",
        "    # restarting episode - reset episode reward and step\n",
        "    episode_reward = 0\n",
        "    step = 1\n",
        "\n",
        "    # reset env and get current state\n",
        "    current_state = env.reset()\n",
        "\n",
        "    # reset flag and iterate until end of episode\n",
        "    done = False\n",
        "    while not done:\n",
        "        # query a model for q-values rather than table\n",
        "        if np.random.random() > epsilon:\n",
        "            action = np.argmax(agent.get_qs(current_state))\n",
        "        else:\n",
        "            action = np.random.randint(0, ACTION_SPACE_SIZE)\n",
        "        \n",
        "        new_state, reward, done = env.step(action)\n",
        "\n",
        "        # transform continuous state to discrete and count reward\n",
        "        episode_reward += reward\n",
        "\n",
        "        # update replay memory and train main network\n",
        "        agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
        "        agent.train(done, step)\n",
        "        current_state = new_state\n",
        "        step += 1\n",
        "\n",
        "# now we have to train\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyyrQnMHJNqP",
        "colab_type": "text"
      },
      "source": [
        "We need a q-table to store the values "
      ]
    }
  ]
}